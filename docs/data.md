# Сбор и подготовка данных

- Основой системы стала выгрузка с сайта [flibusta](https://flibusta.site/)
- Было обработано более 500,000 книг, включая их аннотации, метаданные и уникальные идентификаторы
- Аннотации и метаданные были сохранены в текстовых и JSON-форматах
## База данных
1) Была создана база данных MySQL на основе выгруженных файлов с сайта [flibusta](https://flibusta.site/) 
2) На базе FAISS была построена векторная база данных *(Данные MySQL БД (из пункта 1) использовались для заполнения этой векторной БД)*
3) Документы содержат аннотации книг, метаданные и ключевые слова
## Обработка данных
- Использовалась библиотека `BeautifulSoup` для очистки аннотаций от HTML-тегов
- Для каждой книги создавался JSON-файл с метаинформацией, включая:
  - название
  - автора
  - год издания
  - язык
  - жанры
  - уникальный ID
- Аннотации книг и их метаданные объединялись для формирования векторного индекса (FAISS-индекса)
## Обработка жанров
- Предзагрузка жанров из базы данных и маппинг на книги.
- У книг без жанров указывалось «Жанр не указан».
## Извлечение ключевых слов
- Извлекаются ключевые слова (как из метаданных для каждой книги, так и для запроса пользователя)
- Для выделения ключевых слов используется KeyBERT на основе модели `paraphrase-multilingual-MiniLM-L12-v2`
## Создание FAISS-индекса:
- Индекс создавался батчами с использованием функций [`create_faiss_index_in_batches`](../book_assist.py) и [`batch_extract_keywords`](../book_assist.py)
- Каждый документ обогащался следующими данными:
  - название
  - автор
  - жанры
  - аннотация
  - ключевые слова
  - другие метаданные
- Итоговый индекс сохранён локально и используется при обработке запросов
> Ключевая особенность индекса: обогащение документов информацией из полей метаданных и ключевых слов значительно повысило качество ретрива
## Парсинг обложек
### Алгоритм загрузки:
- Использовался BeautifulSoup для парсинга страниц книг на [flibusta](https://flibusta.site/)
- Для поиска обложек использовались тег `<img>` и атрибут `alt="Cover image"`
- Обложки сохранялись  в локальной директории
### Оптимизация:
- Применялась многопоточность через `ThreadPoolExecutor`, использовалось 20 потоков
### Результат:
- Загружено почти 400,000 обложек
- Общий объем данных: ~90 ГБ
## Проблемы в данных
> **Короткие аннотации**: У значительной части книг аннотации слишком короткие и малоинформативные. Это негативно повлияло на полноту и качество выдачи рекомендаций
